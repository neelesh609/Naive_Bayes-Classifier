{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nimport tarfile\nimport matplotlib.pyplot as plt\nimport scipy as sp\nimport csv\nimport random\nimport math\nimport operator\nimport os\nfrom collections import Counter\nimport nltk\nfrom sklearn.model_selection import train_test_split\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\namazon_dataframe = pd.read_csv(\"/kaggle/input/sentiment-labelled-sentences-data-set/sentiment labelled sentences/sentiment labelled sentences/amazon_cells_labelled.txt\", \n                        delimiter='\\t', names=['review', 'sentiment'])\n\nyelp_dataframe = pd.read_csv(\"/kaggle/input/sentiment-labelled-sentences-data-set/sentiment labelled sentences/sentiment labelled sentences/yelp_labelled.txt\", \n                        delimiter='\\t', names=['review', 'sentiment'])\n\ndf_train = pd.DataFrame()\ndf_dev = pd.DataFrame()\ndf_test = pd.DataFrame()\n\nframe = [amazon_dataframe, yelp_dataframe]\nfinal_dataframe = pd.concat(frame) # combining the data from amazon and yelp inorder to increase the accuracy ( just a try.)\n\nfinal_dataframe.reset_index(drop='True', inplace=True)\n\nsplit_1 = int(0.8 * len(final_dataframe))\nsplit_2 = int(0.9 * len(final_dataframe))\ndf_train = final_dataframe[:split_1]\ndf_dev = final_dataframe[split_1:split_2]\ndf_test = final_dataframe[split_2:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Combining the data increased the accuracy from 67% to 69.5%","metadata":{}},{"cell_type":"code","source":"\n\ndf_train_len = len(df_train)\nprint('total train records:', df_train_len)\n\ndf_train_pos_len = len(df_train[df_train['sentiment'] == 1])\ndf_train_neg_len = len(df_train[df_train['sentiment'] == 0])\nprob_pos_train = df_train_pos_len / df_train_len\nprob_neg_train = df_train_neg_len / df_train_len\n\nprint ('positive records:', df_train_pos_len)\nprint ('negative records:', df_train_neg_len)\n\nprint ('prob positive records:', prob_pos_train)\nprint ('prob negative records:', prob_neg_train)\n\nprint()\nprint('total dev records:', len(df_dev))\nprint('total test records:', len(df_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf_train.groupby('sentiment').count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf_dev.groupby('sentiment').count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf_test.groupby('sentiment').count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nnp.random.seed(0)\ndf_train = df_train.reindex(np.random.permutation(df_train.index))\ndf_train.to_csv('movie_data_train.csv', index=False, encoding = 'utf-8')\n\nnp.random.seed(0)\ndf_dev = df_dev.reindex(np.random.permutation(df_dev.index))\ndf_dev.to_csv('movie_data_dev.csv', index=False, encoding = 'utf-8')\n\nnp.random.seed(0)\ndf_test = df_test.reindex(np.random.permutation(df_test.index))\ndf_test.to_csv('movie_data_test.csv', index=False, encoding = 'utf-8')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print train data\ndf_train = pd.read_csv('movie_data_train.csv', encoding = 'utf-8')\ndf_train.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf_dev = pd.read_csv('movie_data_dev.csv', encoding = 'utf-8')\ndf_dev.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf_test = pd.read_csv('movie_data_test.csv', encoding = 'utf-8')\ndf_test.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndoc_len = len(df_train)\nprint(doc_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cleaning the data \n","metadata":{}},{"cell_type":"code","source":"\n\ndf_train.columns = df_train.columns.str.strip()         \ndf_train.columns = df_train.columns.str.replace(r\"[^a-zA-Z\\d\\_]+\", \"\")    \ndf_train.columns = df_train.columns.str.replace(r\"[^a-zA-Z\\d\\_]+\", \"\")\n\ndf_dev.columns = df_dev.columns.str.strip()         \ndf_dev.columns = df_dev.columns.str.replace(r\"[^a-zA-Z\\d\\_]+\", \"\")    \ndf_dev.columns = df_dev.columns.str.replace(r\"[^a-zA-Z\\d\\_]+\", \"\")\n\ndf_test.columns = df_test.columns.str.strip()         \ndf_test.columns = df_test.columns.str.replace(r\"[^a-zA-Z\\d\\_]+\", \"\")    \ndf_test.columns = df_test.columns.str.replace(r\"[^a-zA-Z\\d\\_]+\", \"\")\n\ndf_train = df_train.replace([\";\",\":\",\"=\",\"\\+\",\"<\", \">\", \"\\?\", \"!\", \"\\\\\\\\\", \"@\", \"#\", \"$\", \"\\*\", \"%\", \",\", \"\\.\", \"\\(\", \"\\)\", \"\\[\", \"\\]\", \"\\{\", \"\\}\", \"\\\"\", \"/br\"], \"\", regex = True)\ndf_dev = df_dev.replace([\";\",\":\",\"=\",\"\\+\",\"<\", \">\", \"\\?\", \"!\", \"\\\\\\\\\", \"@\", \"#\", \"$\", \"\\*\", \"%\", \",\", \"\\.\", \"\\(\", \"\\)\", \"\\[\", \"\\]\", \"\\{\", \"\\}\", \"\\\"\", \"/br\"], \"\", regex = True)\ndf_test = df_test.replace([\";\",\":\",\"=\",\"\\+\",\"<\", \">\", \"\\?\", \"!\", \"\\\\\\\\\", \"@\", \"#\", \"$\", \"\\*\", \"%\", \",\", \"\\.\", \"\\(\", \"\\)\", \"\\[\", \"\\]\", \"\\{\", \"\\}\", \"\\\"\", \"/br\"], \"\", regex = True)\n\ndf_train = df_train.replace([\"' \", \" '\"], \" \", regex = True)\ndf_dev = df_dev.replace([\"' \", \" '\"], \" \", regex = True)\ndf_test = df_test.replace([\"' \", \" '\"], \" \", regex = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below we Build a vocabulary as list. ","metadata":{}},{"cell_type":"code","source":"\n\nwordfreq = dict()\nwordfreq_pos = dict()\nwordfreq_neg = dict()\nfor ind in df_train.index:\n    review_set = set(df_train['review'][ind].lower().split())\n    for word in review_set:\n        if word in wordfreq:\n            wordfreq[word] += 1\n        else:\n            wordfreq[word] = 1\n        \n        if df_train['sentiment'][ind] == 1:\n            if word in wordfreq_pos:\n                wordfreq_pos[word] += 1\n            else:\n                wordfreq_pos[word] = 1\n        else:\n            if word in wordfreq_neg:\n                wordfreq_neg[word] += 1\n            else:\n                wordfreq_neg[word] = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we omit rare words for example if the occurrence is less than five times","metadata":{}},{"cell_type":"code","source":"\n\nfinal_vocab = dict()\nfinal_vocab_pos = dict()\nfinal_vocab_neg = dict()\nfor word in wordfreq:\n    if wordfreq[word] > 5:\n        final_vocab[word] = wordfreq[word]\n    if word in wordfreq_pos:\n        if wordfreq_pos[word] > 5:\n            final_vocab_pos[word] = wordfreq_pos[word]\n    if word in wordfreq_neg:\n        if wordfreq_neg[word] > 5:\n            final_vocab_neg[word] = wordfreq_neg[word]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint(\"Total Vocab:\",len(final_vocab))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Calculate the following probability**\n\na)Probability of the occurrence\nb)Conditional probability based on the sentiment\n****","metadata":{}},{"cell_type":"code","source":"\nprob_word = dict()\nprob_word_g_pos = dict()\nprob_word_g_neg = dict()\n\nfor word in final_vocab:\n    prob_word[word] = final_vocab[word] / doc_len\n    if word in final_vocab_pos:\n        prob_word_g_pos[word] = final_vocab_pos[word] / df_train_pos_len\n        \n    if word in final_vocab_neg:\n        prob_word_g_neg[word] = final_vocab_neg[word] / df_train_neg_len","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Calculate accuracy using dev dataset** \n\n* Conduct five fold cross validation","metadata":{}},{"cell_type":"code","source":"\naccuracy_normal = []\ndf_dev_arr = np.array_split(df_dev, 5)\nctr = 0\n\nprint(\"Accuracy using 5-Fold Cross Validation:\")\n\nfor df in df_dev_arr:\n    count = 0\n    ctr += 1\n    predicted_sentiments = []\n    prob_pos_g_wir = dict()\n    prob_neg_g_wir = dict()\n    \n    for ind in df.index:\n        numPos = 0.00\n        numNeg = 0.00\n        \n        review_set = set(df['review'][ind].lower().split())\n        for word in review_set:\n            if word in prob_word:\n                if word not in prob_word_g_pos:\n                    numNeg = 0\n                elif word not in prob_word_g_neg:\n                    numPos = 0\n                else:\n                    numPos = numPos + math.log(prob_word_g_pos[word])\n                    numNeg = numNeg + math.log(prob_word_g_neg[word])\n                            \n        prob_pos_g_wir[ind] = pow(math.e, numPos) * prob_pos_train\n        prob_neg_g_wir[ind] = pow(math.e, numNeg) * prob_neg_train\n                            \n        if(prob_pos_g_wir[ind] < prob_neg_g_wir[ind]):\n            predicted_sentiments.append(0)\n        else:\n            predicted_sentiments.append(1)\n                                    \n    df['prediction'] = predicted_sentiments\n                                                                        \n    for ind in df.index:\n        if df['sentiment'][ind] == df['prediction'][ind]:\n            count += 1\n                                            \n    accuracy = count / len(df)\n    accuracy_normal.append(accuracy)\n    print (ctr,\": Accuracy df_dev:\",accuracy*100,\"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Compare the effect of Smoothing**","metadata":{}},{"cell_type":"code","source":"\nprob_word_g_pos_smooth = dict()\nprob_word_g_neg_smooth = dict()\n\nfor word in final_vocab:\n    if word in final_vocab_pos:\n        prob_word_g_pos_smooth[word] = (final_vocab_pos[word]+1) / (df_train_pos_len + len(final_vocab))\n        \n    if word in final_vocab_neg:\n        prob_word_g_neg_smooth[word] = (final_vocab_neg[word]+1) / (df_train_neg_len + len(final_vocab))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\naccuracy_smooth = []\ndf_dev_arr = np.array_split(df_dev, 5)\nctr = 0\n\nprint(\"Accuracy after Smoothing using 5-Fold Cross Validation:\")\n\nfor df in df_dev_arr:\n    count = 0\n    ctr += 1\n    predicted_sentiments = []\n    prob_pos_g_wir = dict()\n    prob_neg_g_wir = dict()\n    \n    for ind in df.index:\n        numPos = 0.00\n        numNeg = 0.00\n        \n        review_set = set(df['review'][ind].lower().split())\n        for word in review_set:\n            if word in prob_word:\n                if word not in prob_word_g_pos:\n                    numNeg = 0\n                elif word not in prob_word_g_neg:\n                    numPos = 0\n                else:\n                    numPos = numPos + math.log(prob_word_g_pos_smooth[word])\n                    numNeg = numNeg + math.log(prob_word_g_neg_smooth[word])\n                            \n        prob_pos_g_wir[ind] = pow(math.e, numPos) * prob_pos_train\n        prob_neg_g_wir[ind] = pow(math.e, numNeg) * prob_neg_train\n                            \n        if(prob_pos_g_wir[ind] < prob_neg_g_wir[ind]):\n            predicted_sentiments.append(0)\n        else:\n            predicted_sentiments.append(1)\n                                    \n    df['prediction'] = predicted_sentiments\n                                                                        \n    for ind in df.index:\n        if df['sentiment'][ind] == df['prediction'][ind]:\n            count += 1\n                                            \n    accuracy = count / len(df)\n    accuracy_smooth.append(accuracy)\n    print (ctr,\": Accuracy df_dev:\",accuracy*100,\"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nbetterNormal = 0\nbetterSmoothing = 0\n\nfor i in range(len(accuracy_normal)):\n    if accuracy_normal[i] > accuracy_smooth[i]:\n        betterNormal += 1\n    else:\n        betterSmoothing +=1\n\nif(betterNormal > betterSmoothing):\n    print(\"For the given dev dataset, accuracy is better without smoothing\")\nelse:\n    print(\"For the given dev dataset, accuracy is better with smoothing\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Derive Top 10 words that predicts positive and negative class\nP[Positive| word] \n**","metadata":{}},{"cell_type":"code","source":"\n\nprob_pos_given_word = dict()\nprob_neg_given_word = dict()\n\nfor word in final_vocab:\n    if word in final_vocab_pos:\n        prob_pos_given_word[word] = (prob_word_g_pos[word] * prob_pos_train) / prob_word[word]\n    if word in final_vocab_neg:\n        prob_neg_given_word[word] = (prob_word_g_neg[word] * prob_neg_train) / prob_word[word]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Positive Words**","metadata":{}},{"cell_type":"code","source":"\nprint(\"Top 10 words predicting positive class:\")\nprob_pos_given_word = sorted(prob_pos_given_word.items(), key=operator.itemgetter(1), reverse=True)\n\nprob_pos_given_word[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Negative Words**","metadata":{}},{"cell_type":"code","source":"\nprint(\"Top 10 words predicting negative class:\")\nprob_neg_given_word = sorted(prob_neg_given_word.items(), key=operator.itemgetter(1), reverse=True)\nprob_neg_given_word[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**calculate the final accuracy using the hyperparameters found from the above step**","metadata":{}},{"cell_type":"code","source":"\npredicted_sentiments = []\nprob_pos_g_wir = dict()\nprob_neg_g_wir = dict()\n\nif(betterNormal < betterSmoothing):\n    prob_word_g_pos = prob_word_g_pos_smooth\n\nfor ind in df_test.index:\n    numPos = 0.00\n    numNeg = 0.00\n    \n    review_set = set(df_test['review'][ind].lower().split())\n    for word in review_set:\n        if word in prob_word:\n            if word not in prob_word_g_pos:\n                numNeg = 0\n            elif word not in prob_word_g_neg:\n                numPos = 0\n            else:\n                numPos = numPos + math.log(prob_word_g_pos[word])\n                numNeg = numNeg + math.log(prob_word_g_neg[word])\n    \n    prob_pos_g_wir[ind] = pow(math.e, numPos) * prob_pos_train\n    prob_neg_g_wir[ind] = pow(math.e, numNeg) * prob_neg_train\n    \n    if(prob_pos_g_wir[ind] < prob_neg_g_wir[ind]):\n        predicted_sentiments.append(0)\n    else:\n        predicted_sentiments.append(1)\n        \ndf_test['prediction'] = predicted_sentiments\n\ncount = 0\n\nfor ind in df_test.index:\n    if df_test['sentiment'][ind] == df_test['prediction'][ind]:\n        count += 1\n        \naccuracy = count / len(df_test)\nprint (\"Accuracy df_test:\",accuracy*100,\"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The accuracy is found to be 69.5%","metadata":{}},{"cell_type":"markdown","source":"**References**\n\nhttps://www.dataquest.io/blog/naive-bayes-tutorial/\nhttps://stackoverflow.com/questions/51085553/scikit-learn-5-fold-cross-validation-train-test-split\nhttps://stackoverflow.com/questions/20618804/how-to-smooth-a-curve-in-the-right-way\nhttps://stackabuse.com/the-naive-bayes-algorithm-in-python-with-scikit-learn/\n","metadata":{}}]}